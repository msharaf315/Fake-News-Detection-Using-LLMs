{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_AuDyms1NkI"
      },
      "source": [
        "#Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybQctPGCwzmm"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/Data Science Lab/Datasets/\"\n",
        "\n",
        "train_df = pd.read_csv(root_path + 'train.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n",
        "valid_df = pd.read_csv(root_path + 'valid.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n",
        "test_df = pd.read_csv(root_path + 'test.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNarHjYz1QAK"
      },
      "source": [
        "#Preprocessing\n",
        "Remove punctuation and stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRNWwlL2E42V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKcpHYl-gC2a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRnC0_0w1Vvp"
      },
      "source": [
        "#Create the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOJz9lG96dFC"
      },
      "source": [
        "#Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhfTP0weBBGi"
      },
      "outputs": [],
      "source": [
        "train_dataset = LiarDataset(preprocess_liar_statements(train_df), liar_vocab)\n",
        "validation_dataset = LiarDataset(preprocess_liar_statements(valid_df), liar_vocab)\n",
        "test_dataset = LiarDataset(preprocess_liar_statements(test_df), liar_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fxeDuc7eC3l"
      },
      "source": [
        "#Create the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utquFM-8D-Bk"
      },
      "outputs": [],
      "source": [
        "# Create the baseline model\n",
        "class BiLSTMTextClassifierModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab, embedding_dim, hidden_dim, number_of_labels):\n",
        "    super(BiLSTMTextClassifierModel, self).__init__()\n",
        "    self.number_of_labels = number_of_labels\n",
        "    self.embedding = nn.Embedding(len(vocab), embedding_dim, vocab[\"<pad>\"])\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "    self.top_layer = nn.Linear(2*hidden_dim, self.number_of_labels)\n",
        "    self.softmax = F.softmax\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeddings = self.embedding(x)\n",
        "    rnn_output, _ = self.rnn(embeddings)\n",
        "    last_hidden = rnn_output[:, -1, :]\n",
        "    top_layer_output = self.top_layer(last_hidden)\n",
        "    output = self.softmax(top_layer_output)\n",
        "    return output\n",
        "\n",
        "\n",
        "model = BiLSTMTextClassifierModel(liar_vocab, 300, 128, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83qD-TA6Yp7H"
      },
      "outputs": [],
      "source": [
        "def calculate_evaluation_metrics(correct_predictions, total_test_samples, targets_list, predicted_labels_list):\n",
        "    labels_array = [\"pants-fire\", \"false\", \"barely-true\", \"mostly-true\", \"true\"]\n",
        "    accuracy = correct_predictions / total_test_samples\n",
        "    precision = precision_score(targets_list, predicted_labels_list,average='micro')\n",
        "    recall = recall_score(targets_list, predicted_labels_list, average='micro')\n",
        "    f1 = f1_score(targets_list, predicted_labels_list, average='micro')\n",
        "\n",
        "    print(f'accuray: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qECjKYDhYw2M"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "# TODO refactor\n",
        "# Hyper parameters\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  training_loss = 0\n",
        "  total_training_samples = 0\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "      # set model to training mode\n",
        "      inputs = batch[0]\n",
        "      targets = batch[1]\n",
        "      output = model(inputs)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      number_of_samples = len(batch)\n",
        "      training_loss+=loss.item() * number_of_samples\n",
        "      total_training_samples += number_of_samples\n",
        "# Evaluate on the validation set after every epoch\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  total_validation_samples = 0\n",
        "  correct_validation_predictions = 0\n",
        "  predicted_labels_list = []\n",
        "  targets_list = []\n",
        "  with torch.no_grad():\n",
        "      for inputs in validation_loader:\n",
        "          # create prediction\n",
        "          inputs = batch[0]\n",
        "          targets = batch[1]\n",
        "          outputs = model(inputs)\n",
        "          # calculate the loss\n",
        "          val_loss = loss_function(outputs, targets)\n",
        "          number_of_samples += len(inputs)\n",
        "          val_loss += val_loss.item() * number_of_samples\n",
        "          total_validation_samples += number_of_samples\n",
        "\n",
        "          # calculate f-score\n",
        "          _, predicted_labels = torch.max(outputs, dim=1)\n",
        "          _, true_labels = torch.max(targets, dim=1)\n",
        "          correct_validation_predictions += (predicted_labels == true_labels).sum().item()\n",
        "          predicted_labels_list.extend(predicted_labels.tolist())\n",
        "          targets_list.extend(true_labels.tolist())\n",
        "\n",
        "  print(f\"Epoch {i+1}/{num_epochs}, Train Average Loss: {(training_loss/total_training_samples):.4f}, Val Loss: {(val_loss/total_validation_samples):.4f}\")\n",
        "  calculate_evaluation_metrics(correct_validation_predictions, total_validation_samples, predicted_labels_list, targets_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jekzkPcEbw--"
      },
      "outputs": [],
      "source": [
        "  calculate_evaluation_metrics(correct_validation_predictions, total_validation_samples, predicted_labels_list, targets_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af0qNvDngmFW"
      },
      "outputs": [],
      "source": [
        "# # Use the trained model to make predictions on the test set\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     total_test_samples = 0\n",
        "#     correct_predictions = 0\n",
        "\n",
        "#     predicted_labels_list = []\n",
        "#     targets_list = []\n",
        "\n",
        "#     for inputs in test_loader:\n",
        "#         inputs = batch[0]\n",
        "#         targets = batch[1]\n",
        "#         outputs = model(inputs)\n",
        "\n",
        "#         _, predicted_labels = torch.max(outputs, dim=1)\n",
        "\n",
        "#         total_test_samples += len(inputs)\n",
        "#         correct_predictions += (predicted_labels == targets).sum().item()\n",
        "\n",
        "#         predicted_labels_list.extend(predicted_labels.tolist())\n",
        "#         targets_list.extend(targets.tolist())\n",
        "\n",
        "#     # Calculate evaluation metrics\n",
        "#     accuracy = correct_predictions / total_test_samples\n",
        "#     precision = precision_score(targets_list, predicted_labels_list)\n",
        "#     recall = recall_score(targets_list, predicted_labels_list)\n",
        "#     f1 = f1_score(targets_list, predicted_labels_list)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CchRZvLH3U4B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
