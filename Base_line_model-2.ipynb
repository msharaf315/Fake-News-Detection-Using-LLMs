{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htvDgVQZyYbY",
    "outputId": "7beaf5a2-71f8-405f-fa88-49353e428de4"
   },
   "outputs": [],
   "source": [
    "# !pip3 install torchtext==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmqsqvGIdBcc",
    "outputId": "89b027df-47d8-4660-ea1a-d727963efae3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/root/envs/Team7/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/root/envs/Team7/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/root/envs/Team7/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1639352/2391351146.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/Team7/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/Team7/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import torch.nn.functional as F\n",
    "nltk.download('stopwords')\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_AuDyms1NkI"
   },
   "source": [
    "#Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "buP6u2E5gpbb"
   },
   "outputs": [],
   "source": [
    "# Liar constants\n",
    "LIAR_LABELS_TO_INDEX =  {\n",
    "                            \"pants-fire\": [1.0,0.0,0.0,0.0,0.0,0.0],\n",
    "                            \"false\": [0.0,1.0,0.0,0.0,0.0,0.0],\n",
    "                            \"barely-true\": [0.0,0.0,1.0,0.0,0.0,0.0],\n",
    "                            \"half-true\": [0.0,0.0,0.0,1.0,0.0,0.0],\n",
    "                            \"mostly-true\": [0.0,0.0,0.0,0.0,1.0,0.0],\n",
    "                            \"true\": [0.0,0.0,0.0,0.0,0.0,1.0],\n",
    "                        }\n",
    "\n",
    "LIAR_HEADER = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state_info\", \"party_affiliation\"\n",
    ", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context_venue_or_location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ybQctPGCwzmm"
   },
   "outputs": [],
   "source": [
    "root_path = \"dataset/\"\n",
    "\n",
    "train_df = pd.read_csv(root_path + 'train.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n",
    "valid_df = pd.read_csv(root_path + 'valid.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n",
    "test_df = pd.read_csv(root_path + 'test.tsv', sep='\\t', header=None, names=LIAR_HEADER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNarHjYz1QAK"
   },
   "source": [
    "#Preprocessing\n",
    "Remove punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "05V8ObsmAezw"
   },
   "outputs": [],
   "source": [
    "class PreprocessingConfig():\n",
    "  def __init__(self, remove_quotations: bool = False, remove_punctuation: bool= False, remove_stop_words: bool= False, to_lower_case: bool = False, add_sos_eos_tokens: bool = True):\n",
    "    self.remove_quotations = remove_quotations\n",
    "    self.remove_punctuation = remove_punctuation\n",
    "    self.remove_stop_words = remove_stop_words\n",
    "    self.to_lower_case = to_lower_case\n",
    "    self.add_sos_eos_tokens = add_sos_eos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qRNWwlL2E42V"
   },
   "outputs": [],
   "source": [
    "# TODO replace unknown and nans\n",
    "def preprocess_text(df_series, config: PreprocessingConfig):\n",
    "  if config.remove_quotations:\n",
    "    df_series = df_series.apply(lambda x: re.sub(\"'\", '',x))\n",
    "\n",
    "  if config.remove_punctuation:\n",
    "    punctuation = set(string.punctuation)\n",
    "    df_series  = df_series.apply(lambda x: ''.join(ch for ch in x if ch not in punctuation ))\n",
    "\n",
    "  if config.remove_stop_words:\n",
    "    stop_words = stopwords.words('english')\n",
    "    df_series - df_series.apply(lambda x:  ' '.join([word for word in x.split() if word not in (stop_words)]) )\n",
    "\n",
    "  if config.to_lower_case:\n",
    "    df_series = df_series.apply(lambda x: x.lower())\n",
    "\n",
    "  if config.add_sos_eos_tokens:\n",
    "    df_series  = df_series.apply(lambda x: '<sos> ' + x + \" <eos>\")\n",
    "\n",
    "  return  df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YKcpHYl-gC2a"
   },
   "outputs": [],
   "source": [
    "def preprocess_liar_statements(df):\n",
    "  config = PreprocessingConfig()\n",
    "  df[\"processed_statement\"] = preprocess_text(df[\"statement\"], config)\n",
    "  df[\"labels_index\"] = df[\"label\"].apply(lambda x: LIAR_LABELS_TO_INDEX[x])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRnC0_0w1Vvp"
   },
   "source": [
    "#Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kZCTMKtS1LyL"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(series):\n",
    "    for text in df[\"processed_statement\"]:\n",
    "        yield text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v63FFs16kY5M"
   },
   "outputs": [],
   "source": [
    "df = preprocess_liar_statements(train_df)\n",
    "liar_vocab = build_vocab_from_iterator(yield_tokens(df[\"processed_statement\"]), specials=[\"<unk>\", \"<pad>\"])\n",
    "liar_vocab.set_default_index(liar_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOJz9lG96dFC"
   },
   "source": [
    "#Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IVju3MVopYjX"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "class LiarDataset(Dataset):\n",
    "\n",
    "  def __init__(self, df, vocab, transform=None, target_transform=None):\n",
    "    self.data = df[\"processed_statement\"]\n",
    "    self.labels = df[\"labels_index\"]\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    example = self.data[idx]\n",
    "    label = self.labels[idx]\n",
    "    numerical_tokens = [self.vocab[token] for token in example.split()]\n",
    "\n",
    "    return torch.tensor(numerical_tokens), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rV0xq_3hGW-h"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# We do the padding here because the sentences in each batch should have the same dimension.\n",
    "class MyCollate:\n",
    "    # pad_idx is the index for the <pad> token\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Batch is a tuple of sentences and their respective labels, this can be changed depending on the input fields we are using for training\n",
    "        # by changing the get item function in the dataset class.\n",
    "        sentences = [x[0] for x in batch]\n",
    "        labels = [x[1] for x in batch]\n",
    "        paded_sentences = pad_sequence(sentences, batch_first=True, padding_value = self.pad_idx)\n",
    "        return paded_sentences , torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bhfTP0weBBGi"
   },
   "outputs": [],
   "source": [
    "train_dataset = LiarDataset(preprocess_liar_statements(train_df), liar_vocab)\n",
    "validation_dataset = LiarDataset(preprocess_liar_statements(valid_df), liar_vocab)\n",
    "test_dataset = LiarDataset(preprocess_liar_statements(test_df), liar_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn = MyCollate(pad_idx=liar_vocab[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fxeDuc7eC3l"
   },
   "source": [
    "#Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "utquFM-8D-Bk"
   },
   "outputs": [],
   "source": [
    "# Create the baseline model\n",
    "class BiLSTMTextClassifierModel(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab, embedding_dim, hidden_dim, number_of_labels):\n",
    "    super(BiLSTMTextClassifierModel, self).__init__()\n",
    "    self.number_of_labels = number_of_labels\n",
    "    self.embedding = nn.Embedding(len(vocab), embedding_dim, vocab[\"<pad>\"])\n",
    "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "    self.top_layer = nn.Linear(2*hidden_dim, self.number_of_labels)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.softmax = F.softmax\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeddings = self.embedding(x)\n",
    "    rnn_output, _ = self.rnn(embeddings)\n",
    "    last_hidden = rnn_output[:, -1, :]\n",
    "    top_layer_output = self.top_layer(self.relu(last_hidden))\n",
    "    return self.softmax(self.relu(top_layer_output), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qECjKYDhYw2M"
   },
   "outputs": [],
   "source": [
    "# trains one batch, returns total batch loss\n",
    "def train_one_batch(model, inputs, targets, optimizer, loss_function):\n",
    "        # Predict/Forward Pass\n",
    "        predictions = model(inputs)\n",
    "        # Compute loss\n",
    "        loss = loss_function(predictions, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Multiply the cross entropy loss which is the average by the batch size so we get the total loss for the batch, we can divide this by all data set \n",
    "        # Size to get average loss for the epoch\n",
    "        batch_size_train =  len(inputs)\n",
    "        batch_loss = loss.item() * batch_size_train\n",
    "        return batch_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validates one batch, returns total batch loss, number of true positives\n",
    "def validate_one_batch(model, inputs, targets, loss_function):\n",
    "        predictions_val = model(inputs).detach()\n",
    "        loss_validation = loss_function(predictions_val, targets)\n",
    "        \n",
    "        \n",
    "        # calculate average loss\n",
    "        batch_size_val = len(inputs)\n",
    "        batch_loss = loss_validation.item() * batch_size_val\n",
    "        \n",
    "        # Calculate True positives\n",
    "        predicted_class = predictions_val.argmax(axis=1)\n",
    "        correct_class = targets.argmax(axis=1)\n",
    "    \n",
    "        true_positives_count = sum(predicted_class == correct_class).item()\n",
    "        return batch_loss, true_positives_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Af0qNvDngmFW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, average train loss: 1.7836974520236253, average val loss: 1.7813379786838994, val accuracy: 0.21573208722741433,  training time : 2.9156482219696045\n",
      "epoch 2, average train loss: 1.7834785602986813, average val loss: 1.789648190094303, val accuracy: 0.19470404984423675,  training time : 2.095219373703003\n",
      "epoch 3, average train loss: 1.7815971709787846, average val loss: 1.7851098293084593, val accuracy: 0.205607476635514,  training time : 1.9737038612365723\n",
      "epoch 4, average train loss: 1.7747184906154871, average val loss: 1.7953519204695276, val accuracy: 0.220404984423676,  training time : 1.861588716506958\n",
      "epoch 5, average train loss: 1.7767615742981433, average val loss: 1.7919336234297707, val accuracy: 0.1923676012461059,  training time : 2.1257686614990234\n",
      "epoch 6, average train loss: 1.762551800906658, average val loss: 1.785548113588232, val accuracy: 0.21261682242990654,  training time : 2.2622108459472656\n",
      "epoch 7, average train loss: 1.7333800189197064, average val loss: 1.783383581497216, val accuracy: 0.21105919003115264,  training time : 1.7445361614227295\n",
      "epoch 8, average train loss: 1.7198796544224024, average val loss: 1.7822311258761683, val accuracy: 0.2118380062305296,  training time : 1.970538854598999\n",
      "epoch 9, average train loss: 1.7120184708386659, average val loss: 1.7896801853477027, val accuracy: 0.220404984423676,  training time : 1.9988033771514893\n",
      "epoch 10, average train loss: 1.6952148728072642, average val loss: 1.7939530079981247, val accuracy: 0.22274143302180685,  training time : 2.8695576190948486\n",
      "epoch 11, average train loss: 1.6726447761058807, average val loss: 1.8042985965901075, val accuracy: 0.19003115264797507,  training time : 2.3767709732055664\n",
      "epoch 12, average train loss: 1.650312877073884, average val loss: 1.8068197349150232, val accuracy: 0.1939252336448598,  training time : 2.0734121799468994\n",
      "epoch 13, average train loss: 1.6275198075920343, average val loss: 1.806782593608274, val accuracy: 0.1939252336448598,  training time : 1.534726858139038\n",
      "epoch 14, average train loss: 1.6136712346225976, average val loss: 1.806359382433312, val accuracy: 0.18925233644859812,  training time : 1.5026435852050781\n",
      "epoch 15, average train loss: 1.6011383555829526, average val loss: 1.8127681807936908, val accuracy: 0.19314641744548286,  training time : 1.513542890548706\n",
      "epoch 16, average train loss: 1.591320164501667, average val loss: 1.8033176627114555, val accuracy: 0.1853582554517134,  training time : 1.8337109088897705\n",
      "epoch 17, average train loss: 1.5811531566083432, average val loss: 1.804421475371839, val accuracy: 0.18769470404984423,  training time : 2.6661765575408936\n",
      "epoch 18, average train loss: 1.5738161761313676, average val loss: 1.8142449291324318, val accuracy: 0.17523364485981308,  training time : 1.7475385665893555\n",
      "epoch 19, average train loss: 1.5612924844026566, average val loss: 1.8125059805183767, val accuracy: 0.17990654205607476,  training time : 1.7925500869750977\n",
      "epoch 20, average train loss: 1.5575634501874447, average val loss: 1.8101420201987863, val accuracy: 0.1838006230529595,  training time : 1.5985918045043945\n",
      "epoch 21, average train loss: 1.5502046450972558, average val loss: 1.8052640258337478, val accuracy: 0.18146417445482865,  training time : 1.8039588928222656\n",
      "epoch 22, average train loss: 1.5430880170315504, average val loss: 1.8086511290333354, val accuracy: 0.1806853582554517,  training time : 1.660642385482788\n",
      "epoch 23, average train loss: 1.5417344480752946, average val loss: 1.8118518880594556, val accuracy: 0.17445482866043613,  training time : 1.5666418075561523\n",
      "epoch 24, average train loss: 1.5398396078497172, average val loss: 1.8138889828203624, val accuracy: 0.17289719626168223,  training time : 1.875861644744873\n",
      "epoch 25, average train loss: 1.5372732620686294, average val loss: 1.8113010070777014, val accuracy: 0.17133956386292834,  training time : 1.4383385181427002\n",
      "epoch 26, average train loss: 1.5345146082341672, average val loss: 1.8133838496104209, val accuracy: 0.17601246105919002,  training time : 1.77656888961792\n",
      "epoch 27, average train loss: 1.5318866170942784, average val loss: 1.8122187091554065, val accuracy: 0.1674454828660436,  training time : 2.682959794998169\n",
      "epoch 28, average train loss: 1.5256092593073844, average val loss: 1.8123047344409788, val accuracy: 0.1658878504672897,  training time : 2.629415512084961\n",
      "epoch 29, average train loss: 1.52357855476439, average val loss: 1.8141002714448258, val accuracy: 0.1674454828660436,  training time : 2.0223922729492188\n",
      "epoch 30, average train loss: 1.5168928731232882, average val loss: 1.8083299528401218, val accuracy: 0.16978193146417445,  training time : 3.2560510635375977\n",
      "epoch 31, average train loss: 1.516263635829091, average val loss: 1.811348735357742, val accuracy: 0.1674454828660436,  training time : 2.796536445617676\n",
      "epoch 32, average train loss: 1.5133220169693231, average val loss: 1.802393560468965, val accuracy: 0.17289719626168223,  training time : 2.459878444671631\n",
      "epoch 33, average train loss: 1.5107690539211034, average val loss: 1.8099191946404003, val accuracy: 0.1588785046728972,  training time : 2.4607694149017334\n",
      "epoch 34, average train loss: 1.5103980414569378, average val loss: 1.813211091953646, val accuracy: 0.16277258566978192,  training time : 1.9548640251159668\n",
      "epoch 35, average train loss: 1.5089611124247313, average val loss: 1.8073996064076172, val accuracy: 0.1705607476635514,  training time : 1.5970332622528076\n",
      "epoch 36, average train loss: 1.5074836786836385, average val loss: 1.8104622750267434, val accuracy: 0.16666666666666666,  training time : 1.600654125213623\n",
      "epoch 37, average train loss: 1.5081117205321788, average val loss: 1.7986956905353106, val accuracy: 0.16666666666666666,  training time : 1.6038718223571777\n",
      "epoch 38, average train loss: 1.5060303591191768, average val loss: 1.8005002105718833, val accuracy: 0.1588785046728972,  training time : 1.5954725742340088\n",
      "epoch 39, average train loss: 1.5095489561557769, average val loss: 1.806255939964936, val accuracy: 0.15654205607476634,  training time : 2.1023998260498047\n",
      "epoch 40, average train loss: 1.504592676833272, average val loss: 1.8052860795523147, val accuracy: 0.16510903426791276,  training time : 2.1902811527252197\n",
      "epoch 41, average train loss: 1.5017394434660674, average val loss: 1.8119110348068665, val accuracy: 0.15654205607476634,  training time : 2.375734329223633\n",
      "epoch 42, average train loss: 1.5005548540502787, average val loss: 1.801446746814288, val accuracy: 0.15342679127725856,  training time : 2.588308811187744\n",
      "epoch 43, average train loss: 1.4984779119491578, average val loss: 1.8054332621743745, val accuracy: 0.16355140186915887,  training time : 2.881845235824585\n",
      "epoch 44, average train loss: 1.50011434443295, average val loss: 1.8024848443325434, val accuracy: 0.16822429906542055,  training time : 2.120626449584961\n",
      "epoch 45, average train loss: 1.5009448301047086, average val loss: 1.805146282707048, val accuracy: 0.16199376947040497,  training time : 2.2838985919952393\n",
      "epoch 46, average train loss: 1.497611588984728, average val loss: 1.8027583594634153, val accuracy: 0.1573208722741433,  training time : 2.1963319778442383\n",
      "epoch 47, average train loss: 1.4965124517679214, average val loss: 1.8112794521070343, val accuracy: 0.16199376947040497,  training time : 1.9344134330749512\n",
      "epoch 48, average train loss: 1.4942616827785968, average val loss: 1.808003242884841, val accuracy: 0.15965732087227413,  training time : 1.5397045612335205\n",
      "epoch 49, average train loss: 1.498847357928753, average val loss: 1.8052169853281752, val accuracy: 0.16277258566978192,  training time : 1.4968359470367432\n",
      "epoch 50, average train loss: 1.4970450125634671, average val loss: 1.8070317139135343, val accuracy: 0.1573208722741433,  training time : 2.070423126220703\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = BiLSTMTextClassifierModel(liar_vocab, 300, 128, 6)\n",
    "# Training params\n",
    "num_epochs = 50\n",
    "# Hyper parameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda:6\")\n",
    "model.to(device)\n",
    "loss_fn.to(device)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_accuracy_history = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0. \n",
    "    total_data_points_train = 0\n",
    "    for train_batch in train_loader:\n",
    "        inputs = train_batch[0].to(device)\n",
    "        targets = train_batch[1].to(device)\n",
    "        \n",
    "        train_batch_loss = train_one_batch(model, inputs, targets, optimizer, loss_fn)\n",
    "        \n",
    "        total_train_loss += train_batch_loss\n",
    "        total_data_points_train += len(inputs)\n",
    "    \n",
    "    \n",
    "    average_epoch_loss_train = total_train_loss/total_data_points_train\n",
    "    train_loss.append(average_epoch_loss_train)\n",
    "    \n",
    "    # Validation\n",
    "    # TODO restructure\n",
    "    model.eval()        \n",
    "    total_val_loss = 0. \n",
    "    total_data_points_val = 0\n",
    "    true_positives_val = 0\n",
    "    for validation_batch in validation_loader:\n",
    "        inputs_val = validation_batch[0].to(device)\n",
    "        targets_val = validation_batch[1].to(device)\n",
    "        \n",
    "        # Returns loss and true positives count\n",
    "        batch_loss_val, true_positives_count = validate_one_batch(model, inputs_val, targets_val, loss_fn)\n",
    "            \n",
    "        # calculate average loss and appends true positives count\n",
    "        total_val_loss += batch_loss_val\n",
    "        total_data_points_val += len(inputs_val)\n",
    "        true_positives_val += true_positives_count\n",
    "    \n",
    "    average_epoch_loss_val = total_val_loss/total_data_points_val\n",
    "    val_accuracy = true_positives_val/total_data_points_val\n",
    "    val_accuracy_history.append(val_accuracy)    \n",
    "    val_loss.append(average_epoch_loss_val)\n",
    "    \n",
    "        \n",
    "    # Print every epoch's metrics\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"epoch {epoch + 1}, average train loss: {average_epoch_loss_train}, average val loss: {average_epoch_loss_val}, val accuracy: {val_accuracy},  training time : {elapsed_time}\")\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test loss: 1.8184774796983438, accuracy: 0.1499605367008682\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()        \n",
    "total_test_loss = 0. \n",
    "total_data_points_test = 0\n",
    "true_positives_test = 0\n",
    "i=0\n",
    "for test_batch in test_loader:\n",
    "    inputs_test = test_batch[0].to(device)\n",
    "    targets_test = test_batch[1].to(device)\n",
    "    # Returns loss and true positives count\n",
    "    batch_loss_test, true_positives_count = validate_one_batch(model, inputs_test, targets_test, loss_fn)\n",
    "            \n",
    "    # calculate average loss and appends true positives count\n",
    "    total_test_loss += batch_loss_test\n",
    "    total_data_points_test += len(inputs_test)\n",
    "    true_positives_test += true_positives_count\n",
    "    \n",
    "average_epoch_loss_test = total_test_loss/total_data_points_test\n",
    "accuracy_test = true_positives_test/total_data_points_test\n",
    "print(f\"average test loss: {average_epoch_loss_test}, accuracy: {accuracy_test}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
